# Python web parser Test Task

### Задача

Написать парсер новостей c сайта ByBit: https://announcements.bybit.com/en-US/?category=&page=1

Парсер должен:
1. Выполнять JS
2. Запрашивать свежие данные 1 раз в секунду (обходить возможные блокировки и кеширование CDN). 

Если появилась новая новость, то сохранять точное время её появления, заголовок и ссылку на неё в .csv

_________

### Отчет по задаче

Проект выполнен при помощи комбинации библиотек `requests`, `beautifulsoup4` и `fake-useragent`.

Парсер имеет 2 консольные команды:

- `init_update`

  (Команда для первичного запуска парсера. Выполняется создание `csv` файла 
(если еще не создан) и производится парсинг ВСЕХ новостей с указанного сайта.
Можно использовать в случае добавления большого количества новых новостей на сайт
(более 8). В таком случае в старый файл будут добавлены ВСЕ новые новости
(новые - новости, заголовок которых отличается от заголовка уже сохраненных новостей))
- `live_update`

  (Команда для начала отслеживания новых новостей в режиме реального времени. 
При запуске этой команды первая страница новостей начинает
отслеживаться на предмет появления новых записей.
Запросы отправляются с перерывом в одну секунду. Если `csv` файл уже создан, происходит
сравнение заголовков новостей с первой страницы сайта с заголовками уже сохраненных
новостей. Если на сайте появляется новая новость - она записывается в конец `csv` файла.
Если `csv` файл отсутствует - он создается и в него записываются все новосте ТОЛЬКО с
первой страницы сайта. Далее парсер продолжает в штатном режиме отслеживать изменения на сайте.)

(Команды вводятся как параметр к основному исполняемому файлу `parser.py`. 
Пример: `python parser.py init_update`)

JS успешно выполняется на сайте и предоставляет данные для получения. 
Использование `selenium` как средства для получения динамической информации,
в реалиях этого проекта, мне показалось избыточным.

Избегание кеширования реализовано передачей заголовков,
запрещающих кешировать данные и передачей динамически генерируемого `get`
параметра в ссылке.

Минимальный уровень обхода различных блокировок от роботов реализован
передачей ложных заголовков `User-Agent`. Заголовки динамически генерируются на каждый
отдельный запрос благодаря библиотеке `fake-useragent`.

Добавлена возможность использования `Proxy`. Для этого необходимо в
`config.py` в параметре `proxy_pass` указать `True` и в параметре
`proxies` указать адреса `proxy` серверов.

По сути готова простая система логирования. Достаточно подключить логгер и заменить вывод в консоль (`print`)
на вывод в логгер с необходимым уровнем

_________

### Установка проекта

1) В директории приложения создать виртуальное окружение `Python` (`python3 -m venv venv`)
2) Активировать виртуальное окружение (Win: `.\venv\Scripts\activate`, Linux: `source venv/bin/activate`)
3) Установить зависимости из файла `requirements.txt` (`pip install -r requirements.txt`)

_________

